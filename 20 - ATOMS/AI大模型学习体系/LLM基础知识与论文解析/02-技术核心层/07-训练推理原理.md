# 训练推理原理

> [!info] **深度理解**：大语言模型训练和推理的核心原理

## 🧠 训练原理深度解析

### 1. 神经网络基础
```python
class NeuralNetworkBasics:
    @staticmethod
    def forward_pass(X, weights, bias, activation='relu'):
        """
        前向传播
        """
        # 线性变换
        Z = torch.matmul(X, weights.T) + bias
        
        # 激活函数
        if activation == 'relu':
            A = torch.relu(Z)
        elif activation == 'sigmoid':
            A = torch.sigmoid(Z)
        elif activation == 'tanh':
            A = torch.tanh(Z)
        elif activation == 'gelu':
            A = torch.nn.functional.gelu(Z)
        else:
            A = Z
            
        return A, Z
    
    @staticmethod
    def backward_pass(dA, Z, activation='relu'):
        """
        反向传播
        """
        # 激活函数梯度
        if activation == 'relu':
            dZ = dA * (Z > 0).float()
        elif activation == 'sigmoid':
            s = torch.sigmoid(Z)
            dZ = dA * s * (1 - s)
        elif activation == 'tanh':
            t = torch.tanh(Z)
            dZ = dA * (1 - t**2)
        elif activation == 'gelu':
            dZ = dA * torch.nn.functional.gelu(Z).backward()
        else:
            dZ = dA
            
        return dZ
```

### 2. 损失函数详解
```python
class LossFunctions:
    @staticmethod
    def cross_entropy_loss(predictions, targets):
        """
        交叉熵损失
        """
        # Softmax
        softmax_preds = torch.softmax(predictions, dim=-1)
        
        # 计算交叉熵
        loss = -torch.sum(targets * torch.log(softmax_preds + 1e-8), dim=-1)
        
        return loss.mean()
    
    @staticmethod
    def mse_loss(predictions, targets):
        """
        均方误差损失
        """
        return torch.mean((predictions - targets) ** 2)
    
    @staticmethod
    def kl_divergence_loss(p, q):
        """
        KL散度损失
        """
        return torch.sum(p * torch.log(p / (q + 1e-8)), dim=-1).mean()
    
    @staticmethod
    def contrastive_loss(embeddings1, embeddings2, labels, temperature=0.1):
        """
        对比学习损失
        """
        # 计算相似度矩阵
        similarity_matrix = torch.matmul(embeddings1, embeddings2.T) / temperature
        
        # 计算对比损失
        exp_sim = torch.exp(similarity_matrix)
        positive_pairs = exp_sim.diag()
        negative_pairs = exp_sim.sum(dim=1) - positive_pairs
        
        loss = -torch.log(positive_pairs / (positive_pairs + negative_pairs))
        
        return loss.mean()
```

### 3. 优化算法深入
```python
class Optimizers:
    def __init__(self, params, lr=0.001):
        self.params = list(params)
        self.lr = lr
        
    def zero_grad(self):
        """
        清零梯度
        """
        for param in self.params:
            if param.grad is not None:
                param.grad.zero_()
    
    def step(self):
        """
        参数更新步骤
        """
        raise NotImplementedError

class Adam(Optimizers):
    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):
        super().__init__(params, lr)
        self.betas = betas
        self.eps = eps
        self.weight_decay = weight_decay
        
        # 初始化动量变量
        self.m = [torch.zeros_like(param) for param in self.params]
        self.v = [torch.zeros_like(param) for param in self.params]
        self.t = 0
    
    def step(self):
        """
        Adam优化器步骤
        """
        self.t += 1
        
        for i, param in enumerate(self.params):
            if param.grad is None:
                continue
                
            grad = param.grad
            
            # 添加权重衰减
            if self.weight_decay != 0:
                grad = grad + self.weight_decay * param
            
            # 更新一阶矩估计
            self.m[i] = self.betas[0] * self.m[i] + (1 - self.betas[0]) * grad
            
            # 更新二阶矩估计
            self.v[i] = self.betas[1] * self.v[i] + (1 - self.betas[1]) * grad**2
            
            # 偏差校正
            m_hat = self.m[i] / (1 - self.betas[0]**self.t)
            v_hat = self.v[i] / (1 - self.betas[1]**self.t)
            
            # 参数更新
            param.data = param.data - self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)

class AdamW(Optimizers):
    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01):
        super().__init__(params, lr)
        self.betas = betas
        self.eps = eps
        self.weight_decay = weight_decay
        
        # 初始化动量变量
        self.m = [torch.zeros_like(param) for param in self.params]
        self.v = [torch.zeros_like(param) for param in self.params]
        self.t = 0
    
    def step(self):
        """
        AdamW优化器步骤
        """
        self.t += 1
        
        for i, param in enumerate(self.params):
            if param.grad is None:
                continue
                
            grad = param.grad
            
            # 更新一阶矩估计
            self.m[i] = self.betas[0] * self.m[i] + (1 - self.betas[0]) * grad
            
            # 更新二阶矩估计
            self.v[i] = self.betas[1] * self.v[i] + (1 - self.betas[1]) * grad**2
            
            # 偏差校正
            m_hat = self.m[i] / (1 - self.betas[0]**self.t)
            v_hat = self.v[i] / (1 - self.betas[1]**self.t)
            
            # 参数更新（AdamW的权重衰减是解耦的）
            param.data = param.data - self.lr * (m_hat / (torch.sqrt(v_hat) + self.eps) + self.weight_decay * param)
```

## 🚀 推理原理详解

### 1. 自回归生成
```python
class AutoregressiveGeneration:
    def __init__(self, model, tokenizer, max_length=512):
        self.model = model
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def generate(self, prompt, generation_config):
        """
        自回归文本生成
        """
        # 编码输入
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')
        
        # 生成设置
        temperature = generation_config.get('temperature', 1.0)
        top_p = generation_config.get('top_p', 0.9)
        top_k = generation_config.get('top_k', 50)
        do_sample = generation_config.get('do_sample', True)
        
        # 生成文本
        with torch.no_grad():
            for _ in range(self.max_length - input_ids.shape[1]):
                # 模型推理
                outputs = self.model(input_ids)
                logits = outputs.logits[:, -1, :]
                
                # 应用温度
                if temperature != 1.0:
                    logits = logits / temperature
                
                # Top-k过滤
                if top_k > 0:
                    values, indices = torch.topk(logits, top_k)
                    logits = torch.full_like(logits, float('-inf'))
                    logits.scatter_(1, indices, values)
                
                # Top-p (nucleus) 采样
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
                    
                    # 移除超过top-p的token
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0
                    
                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                    logits[indices_to_remove] = float('-inf')
                
                # 采样下一个token
                if do_sample:
                    probs = torch.softmax(logits, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    next_token = torch.argmax(logits, dim=-1, keepdim=True)
                
                # 添加到输入序列
                input_ids = torch.cat([input_ids, next_token], dim=1)
                
                # 检查是否生成结束符
                if next_token.item() == self.tokenizer.eos_token_id:
                    break
        
        # 解码生成的文本
        generated_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
        
        return generated_text
```

### 2. 束搜索 (Beam Search)
```python
class BeamSearch:
    def __init__(self, model, tokenizer, beam_size=5, max_length=512):
        self.model = model
        self.tokenizer = tokenizer
        self.beam_size = beam_size
        self.max_length = max_length
    
    def generate(self, prompt):
        """
        束搜索生成
        """
        # 编码输入
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')
        
        # 初始化束
        beams = [{
            'input_ids': input_ids,
            'score': 0.0,
            'finished': False
        }]
        
        for step in range(self.max_length - input_ids.shape[1]):
            new_beams = []
            
            # 扩展每个束
            for beam in beams:
                if beam['finished']:
                    new_beams.append(beam)
                    continue
                
                # 模型推理
                with torch.no_grad():
                    outputs = self.model(beam['input_ids'])
                    logits = outputs.logits[:, -1, :]
                
                # 获取top-k候选
                log_probs = torch.log_softmax(logits, dim=-1)
                topk_log_probs, topk_tokens = torch.topk(log_probs, self.beam_size)
                
                # 创建新束
                for i in range(self.beam_size):
                    new_input_ids = torch.cat([
                        beam['input_ids'],
                        topk_tokens[:, i:i+1]
                    ], dim=1)
                    
                    new_score = beam['score'] + topk_log_probs[0, i].item()
                    
                    # 检查是否结束
                    finished = (topk_tokens[0, i].item() == self.tokenizer.eos_token_id)
                    
                    new_beams.append({
                        'input_ids': new_input_ids,
                        'score': new_score,
                        'finished': finished
                    })
            
            # 选择top-k束
            new_beams.sort(key=lambda x: x['score'], reverse=True)
            beams = new_beams[:self.beam_size]
            
            # 检查是否所有束都结束
            if all(beam['finished'] for beam in beams):
                break
        
        # 返回最佳束
        best_beam = max(beams, key=lambda x: x['score'])
        generated_text = self.tokenizer.decode(best_beam['input_ids'][0], skip_special_tokens=True)
        
        return generated_text
```

### 3. 推理优化技术
```python
class InferenceOptimization:
    @staticmethod
    def kv_cache_optimization(model, input_ids, max_length=512):
        """
        KV缓存优化
        """
        # 初始化KV缓存
        past_key_values = None
        
        for i in range(max_length - input_ids.shape[1]):
            # 模型推理（使用KV缓存）
            with torch.no_grad():
                outputs = model(
                    input_ids=input_ids[:, -1:] if past_key_values is not None else input_ids,
                    past_key_values=past_key_values,
                    use_cache=True
                )
                
                logits = outputs.logits
                past_key_values = outputs.past_key_values
            
            # 采样下一个token
            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)
            input_ids = torch.cat([input_ids, next_token], dim=1)
            
            # 检查是否结束
            if next_token.item() == model.config.eos_token_id:
                break
        
        return input_ids
    
    @staticmethod
    def speculative_decoding(small_model, large_model, input_ids, max_length=512):
        """
        推测解码
        """
        current_input = input_ids
        
        for step in range(max_length - input_ids.shape[1]):
            # 小模型预测多个token
            small_outputs = small_model.generate(
                current_input,
                max_length=min(current_input.shape[1] + 5, max_length),
                do_sample=False
            )
            
            # 大模型验证
            with torch.no_grad():
                large_outputs = large_model(small_outputs)
                large_logits = large_outputs.logits
                
                # 验证小模型的预测
                verified_tokens = []
                for i in range(small_outputs.shape[1] - current_input.shape[1]):
                    predicted_token = small_outputs[0, current_input.shape[1] + i]
                    actual_logit = large_logits[0, current_input.shape[1] + i - 1, predicted_token]
                    
                    # 验证是否正确
                    if actual_logit > torch.log(torch.tensor(0.5)):  # 简化的验证条件
                        verified_tokens.append(predicted_token)
                    else:
                        break
                
                if verified_tokens:
                    # 接受验证通过的token
                    current_input = torch.cat([
                        current_input,
                        torch.tensor(verified_tokens).unsqueeze(0)
                    ], dim=1)
                else:
                    # 大模型重新预测
                    next_token = torch.argmax(large_logits[:, -1, :], dim=-1, keepdim=True)
                    current_input = torch.cat([current_input, next_token], dim=1)
                
                # 检查是否结束
                if next_token.item() == large_model.config.eos_token_id:
                    break
        
        return current_input
```

## 📊 训练稳定性技术

### 1. 梯度裁剪
```python
class GradientClipping:
    @staticmethod
    def clip_gradients(model, max_norm=1.0):
        """
        梯度裁剪
        """
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)
    
    @staticmethod
    def clip_gradients_value(model, clip_value=1.0):
        """
        按值裁剪梯度
        """
        for param in model.parameters():
            if param.grad is not None:
                param.grad.data.clamp_(-clip_value, clip_value)
```

### 2. 学习率调度
```python
class LearningRateScheduler:
    def __init__(self, optimizer, warmup_steps=1000, total_steps=10000):
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.step_count = 0
    
    def step(self):
        """
        更新学习率
        """
        self.step_count += 1
        
        if self.step_count < self.warmup_steps:
            # Warmup阶段
            lr = self.step_count / self.warmup_steps
        else:
            # Decay阶段
            progress = (self.step_count - self.warmup_steps) / (self.total_steps - self.warmup_steps)
            lr = 0.5 * (1 + torch.cos(torch.pi * progress))
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
    
    def get_lr(self):
        """
        获取当前学习率
        """
        return self.optimizer.param_groups[0]['lr']
```

### 3. 混合精度训练
```python
class MixedPrecisionTraining:
    def __init__(self, model, optimizer, scaler=None):
        self.model = model
        self.optimizer = optimizer
        self.scaler = scaler or torch.cuda.amp.GradScaler()
    
    def train_step(self, batch):
        """
        混合精度训练步骤
        """
        # 前向传播（自动混合精度）
        with torch.cuda.amp.autocast():
            outputs = self.model(**batch)
            loss = outputs.loss
        
        # 反向传播（使用scaler）
        self.scaler.scale(loss).backward()
        
        # 梯度裁剪
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        
        # 优化器步骤（使用scaler）
        self.scaler.step(self.optimizer)
        self.scaler.update()
        
        # 清零梯度
        self.optimizer.zero_grad()
        
        return loss.item()
```

## 🎯 产品经理关注点

### 技术选型决策
```markdown
# 训练技术选型
## 大规模训练
- **分布式训练**: 多GPU/TPU并行
- **混合精度**: 减少内存占用，加速训练
- **梯度累积**: 处理大批量数据
- **学习率调度**: 稳定训练过程

## 推理技术选型
## 延迟敏感
- **KV缓存**: 减少重复计算
- **推测解码**: 加速生成过程
- **量化**: 减少计算复杂度
- **批处理**: 提高吞吐量
```

### 成本效益分析
```python
def training_cost_analysis(config):
    """
    训练成本分析
    """
    # 计算训练成本
    compute_cost = config.model_size * config.training_steps * config.gpu_cost_per_hour
    data_cost = config.dataset_size * config.data_cost_per_sample
    human_cost = config.training_hours * config.hourly_rate
    
    total_cost = compute_cost + data_cost + human_cost
    
    # 计算效益
    performance_improvement = config.performance_improvement
    time_savings = config.time_savings
    quality_improvement = config.quality_improvement
    
    total_benefit = performance_improvement + time_savings + quality_improvement
    
    return {
        'total_cost': total_cost,
        'total_benefit': total_benefit,
        'roi': (total_benefit - total_cost) / total_cost,
        'payback_period': total_cost / total_benefit * 12  # 月
    }
```

## 🔗 相关概念

- [[大模型关键技术栈]] - 训练推理在技术栈中的位置
- [[LLM完整生命周期]] - 训练推理在生命周期中的阶段
- [[模型推理优化]] - 推理阶段的性能优化
- [[Transformer架构解析]] - 训练推理的架构基础

## 📝 最佳实践

### 训练实践
```markdown
# 训练最佳实践
1. **数据质量**: 高质量训练数据
2. **超参数调优**: 系统性超参数搜索
3. **监控指标**: 实时监控训练指标
4. **早停机制**: 防止过拟合
```

### 推理实践
```markdown
# 推理最佳实践
1. **缓存机制**: 利用KV缓存等优化
2. **批处理**: 合理设置批处理大小
3. **负载均衡**: 分布式推理负载均衡
4. **监控告警**: 实时监控推理性能
```

---

*标签：#训练推理 #深度学习 #优化技术 #AI产品经理*
*相关项目：[[AI产品经理技术栈项目]]*
*学习状态：#技术原理 🟡 #应用实践 🟡*