# å¤šæ¨¡æ€èåˆæŠ€æœ¯

> [!info] **è·¨æ¨¡æ€å¯¹é½**ï¼šè§£å†³ä¸åŒæ¨¡æ€ä¹‹é—´å·®å¼‚çš„å…³é”®æŠ€æœ¯

## ğŸŒ å¤šæ¨¡æ€èåˆæ¦‚è¿°

```mermaid
graph TB
    A[æ–‡æœ¬æ¨¡æ€] --> D[èåˆå±‚]
    B[å›¾åƒæ¨¡æ€] --> D
    C[éŸ³é¢‘æ¨¡æ€] --> D
    D --> E[å¤šæ¨¡æ€ç†è§£]
    D --> F[è·¨æ¨¡æ€ç”Ÿæˆ]
    D --> G[æ¨¡æ€è½¬æ¢]
```

## ğŸ”§ æ ¸å¿ƒæŠ€æœ¯æ–¹æ³•

### 1. æ—©æœŸèåˆ (Early Fusion)
```python
class EarlyFusion(nn.Module):
    def __init__(self, text_dim=768, image_dim=512, fusion_dim=1024):
        super().__init__()
        self.text_proj = nn.Linear(text_dim, fusion_dim)
        self.image_proj = nn.Linear(image_dim, fusion_dim)
        self.fusion_layer = nn.Sequential(
            nn.Linear(fusion_dim * 2, fusion_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
    def forward(self, text_features, image_features):
        # æŠ•å½±åˆ°åŒä¸€ç©ºé—´
        text_proj = self.text_proj(text_features)
        image_proj = self.image_proj(image_features)
        
        # æ‹¼æ¥ç‰¹å¾
        fused_features = torch.cat([text_proj, image_proj], dim=-1)
        
        # èåˆå±‚
        output = self.fusion_layer(fused_features)
        return output
```

### 2. æ™šæœŸèåˆ (Late Fusion)
```python
class LateFusion(nn.Module):
    def __init__(self, text_dim=768, image_dim=512, output_dim=512):
        super().__init__()
        self.text_encoder = nn.Sequential(
            nn.Linear(text_dim, output_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        self.image_encoder = nn.Sequential(
            nn.Linear(image_dim, output_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        self.fusion_weight = nn.Parameter(torch.ones(2))
        
    def forward(self, text_features, image_features):
        # åˆ†åˆ«å¤„ç†æ¯ä¸ªæ¨¡æ€
        text_encoded = self.text_encoder(text_features)
        image_encoded = self.image_encoder(image_features)
        
        # åŠ æƒèåˆ
        weights = F.softmax(self.fusion_weight, dim=0)
        fused = weights[0] * text_encoded + weights[1] * image_encoded
        
        return fused
```

### 3. è·¨æ¨¡æ€æ³¨æ„åŠ› (Cross-Modal Attention)
```python
class CrossModalAttention(nn.Module):
    def __init__(self, embed_dim=768, num_heads=8):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        # è·¨æ¨¡æ€æ³¨æ„åŠ›
        self.cross_attention = nn.MultiheadAttention(embed_dim, num_heads)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(0.1)
        
        # å‰é¦ˆç½‘ç»œ
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 4),
            nn.ReLU(),
            nn.Linear(embed_dim * 4, embed_dim),
            nn.Dropout(0.1)
        )
        
    def forward(self, query, key, value, key_padding_mask=None):
        # è·¨æ¨¡æ€æ³¨æ„åŠ›
        attn_output, _ = self.cross_attention(
            query, key, value, 
            key_padding_mask=key_padding_mask
        )
        
        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        query = self.norm1(query + self.dropout(attn_output))
        
        # å‰é¦ˆç½‘ç»œ
        ffn_output = self.ffn(query)
        query = self.norm2(query + self.dropout(ffn_output))
        
        return query
```

## ğŸ¯ å…³é”®æŠ€æœ¯æŒ‘æˆ˜

### 1. æ¨¡æ€å·®å¼‚å¯¹é½
```python
class ModalityAlignment(nn.Module):
    def __init__(self, text_dim=768, image_dim=512, shared_dim=512):
        super().__init__()
        self.text_to_shared = nn.Linear(text_dim, shared_dim)
        self.image_to_shared = nn.Linear(image_dim, shared_dim)
        
        # å¯¹æ¯”å­¦ä¹ æŸå¤±
        self.temperature = nn.Parameter(torch.ones([]) * 0.07)
        
    def forward(self, text_features, image_features):
        # æŠ•å½±åˆ°å…±äº«ç©ºé—´
        text_shared = F.normalize(self.text_to_shared(text_features), dim=-1)
        image_shared = F.normalize(self.image_to_shared(image_features), dim=-1)
        
        return text_shared, image_shared
    
    def contrastive_loss(self, text_shared, image_shared):
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        logits = torch.matmul(text_shared, image_shared.transpose(0, 1)) / self.temperature
        
        # å¯¹æ¯”å­¦ä¹ æŸå¤±
        batch_size = text_shared.size(0)
        labels = torch.arange(batch_size).to(text_shared.device)
        
        loss_i2t = F.cross_entropy(logits, labels)
        loss_t2i = F.cross_entropy(logits.transpose(0, 1), labels)
        
        return (loss_i2t + loss_t2i) / 2
```

### 2. ç»Ÿä¸€è¡¨ç¤ºç©ºé—´
```python
class UnifiedRepresentationSpace(nn.Module):
    def __init__(self, modalities=['text', 'image', 'audio'], embed_dim=768):
        super().__init__()
        self.modalities = modalities
        self.embed_dim = embed_dim
        
        # æ¯ä¸ªæ¨¡æ€çš„ç¼–ç å™¨
        self.encoders = nn.ModuleDict({
            modality: nn.Sequential(
                nn.Linear(self.get_modality_dim(modality), embed_dim),
                nn.ReLU(),
                nn.LayerNorm(embed_dim)
            ) for modality in modalities
        })
        
        # æ¨¡æ€ç‰¹å®šçš„æŠ•å½±
        self.projections = nn.ModuleDict({
            modality: nn.Linear(embed_dim, embed_dim) 
            for modality in modalities
        })
        
    def get_modality_dim(self, modality):
        dim_map = {'text': 768, 'image': 512, 'audio': 256}
        return dim_map.get(modality, 512)
    
    def forward(self, modality_features):
        unified_features = {}
        
        for modality, features in modality_features.items():
            if modality in self.encoders:
                encoded = self.encoders[modality](features)
                projected = self.projections[modality](encoded)
                unified_features[modality] = projected
                
        return unified_features
```

## ğŸš€ ä¸»æµå¤šæ¨¡æ€æ¨¡å‹æ¶æ„

### 1. CLIPæ¶æ„
```python
class CLIPModel(nn.Module):
    def __init__(self, text_model='bert-base', image_model='vit-base', embed_dim=512):
        super().__init__()
        
        # æ–‡æœ¬ç¼–ç å™¨
        self.text_encoder = TextEncoder(text_model)
        self.text_projection = nn.Linear(self.text_encoder.output_dim, embed_dim)
        
        # å›¾åƒç¼–ç å™¨
        self.image_encoder = ImageEncoder(image_model)
        self.image_projection = nn.Linear(self.image_encoder.output_dim, embed_dim)
        
        # æ¸©åº¦å‚æ•°
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
        
    def forward(self, text, image):
        # è·å–ç‰¹å¾
        text_features = self.text_encoder(text)
        image_features = self.image_encoder(image)
        
        # æŠ•å½±åˆ°å…±äº«ç©ºé—´
        text_embed = self.text_projection(text_features)
        image_embed = self.image_projection(image_features)
        
        # å½’ä¸€åŒ–
        text_embed = F.normalize(text_embed, dim=-1)
        image_embed = F.normalize(image_embed, dim=-1)
        
        return text_embed, image_embed
    
    def contrastive_loss(self, text_embed, image_embed):
        # è®¡ç®—logits
        logits_per_text = torch.matmul(text_embed, image_embed.t()) * self.logit_scale.exp()
        logits_per_image = logits_per_text.t()
        
        # è®¡ç®—å¯¹æ¯”æŸå¤±
        batch_size = text_embed.shape[0]
        labels = torch.arange(batch_size).to(text_embed.device)
        
        loss_text = F.cross_entropy(logits_per_text, labels)
        loss_image = F.cross_entropy(logits_per_image, labels)
        
        return (loss_text + loss_image) / 2
```

### 2. Flamingoæ¶æ„
```python
class FlamingoModel(nn.Module):
    def __init__(self, vision_encoder, lang_model, adapter_dim=64):
        super().__init__()
        self.vision_encoder = vision_encoder
        self.lang_model = lang_model
        
        # äº¤å‰æ³¨æ„åŠ›é€‚é…å™¨
        self.cross_attention_layers = nn.ModuleList([
            CrossAttentionAdapter(adapter_dim) 
            for _ in range(lang_model.config.num_hidden_layers)
        ])
        
        # é—¨æ§æœºåˆ¶
        self.gating_layers = nn.ModuleList([
            nn.Linear(adapter_dim, 1) 
            for _ in range(lang_model.config.num_hidden_layers)
        ])
        
    def forward(self, vision_x, lang_x):
        # è§†è§‰ç‰¹å¾æå–
        vision_features = self.vision_encoder(vision_x)
        
        # é€å±‚å¤„ç†
        for i, (layer, cross_attn, gate) in enumerate(zip(
            self.lang_model.transformer.layer,
            self.cross_attention_layers,
            self.gating_layers
        )):
            # æ ‡å‡†è¯­è¨€æ¨¡å‹å±‚
            lang_output = layer(lang_x)
            
            # è·¨æ¨¡æ€æ³¨æ„åŠ›
            cross_output = cross_attn(lang_output, vision_features)
            
            # é—¨æ§èåˆ
            gate_weight = torch.sigmoid(gate(lang_output))
            lang_x = lang_output + gate_weight * cross_output
            
        return lang_x
```

## ğŸ“Š æ€§èƒ½è¯„ä¼°æŒ‡æ ‡

### 1. è·¨æ¨¡æ€æ£€ç´¢
```python
def evaluate_cross_modal_retrieval(model, test_loader, device):
    model.eval()
    
    text_to_image_recall = []
    image_to_text_recall = []
    
    with torch.no_grad():
        for batch in test_loader:
            text_features, image_features = model(batch['text'], batch['image'])
            
            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity = torch.matmul(text_features, image_features.t())
            
            # è®¡ç®—Recall@K
            for k in [1, 5, 10]:
                # Text-to-Image
                _, top_k_indices = similarity.topk(k, dim=1)
                recall_k = calculate_recall(top_k_indices, batch['labels'])
                text_to_image_recall.append(recall_k)
                
                # Image-to-Text
                _, top_k_indices = similarity.t().topk(k, dim=1)
                recall_k = calculate_recall(top_k_indices, batch['labels'])
                image_to_text_recall.append(recall_k)
    
    return {
        'text_to_image_recall@1': np.mean(text_to_image_recall),
        'text_to_image_recall@5': np.mean(text_to_image_recall),
        'text_to_image_recall@10': np.mean(text_to_image_recall),
        'image_to_text_recall@1': np.mean(image_to_text_recall),
        'image_to_text_recall@5': np.mean(image_to_text_recall),
        'image_to_text_recall@10': np.mean(image_to_text_recall),
    }
```

### 2. è§†è§‰é—®ç­”
```python
def evaluate_vqa(model, test_loader, device):
    model.eval()
    
    predictions = []
    references = []
    
    with torch.no_grad():
        for batch in test_loader:
            # æ¨¡å‹é¢„æµ‹
            outputs = model(
                image=batch['image'],
                question=batch['question']
            )
            
            # è·å–é¢„æµ‹ç­”æ¡ˆ
            predicted_answers = outputs.predictions
            predicted_answers = [model.tokenizer.decode(ans) for ans in predicted_answers]
            
            predictions.extend(predicted_answers)
            references.extend(batch['answers'])
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = calculate_accuracy(predictions, references)
    
    return {
        'vqa_accuracy': accuracy,
        'bleu_score': calculate_bleu(predictions, references),
        'rouge_score': calculate_rouge(predictions, references)
    }
```

## ğŸ¯ äº§å“ç»ç†å…³æ³¨ç‚¹

### æŠ€æœ¯é€‰å‹å†³ç­–
```markdown
# é€‰å‹æ¡†æ¶
## åº”ç”¨åœºæ™¯åŒ¹é…
- **æ£€ç´¢ä»»åŠ¡**: CLIP, ALIGN
- **ç”Ÿæˆä»»åŠ¡**: DALL-E, Stable Diffusion
- **ç†è§£ä»»åŠ¡**: Flamingo, BLIP
- **å¯¹è¯ä»»åŠ¡**: GPT-4V, Gemini

## æˆæœ¬è€ƒè™‘
- **è®­ç»ƒæˆæœ¬**: å¤§æ¨¡å‹vså°æ¨¡å‹
- **æ¨ç†æˆæœ¬**: APIè°ƒç”¨vsè‡ªå»ºéƒ¨ç½²
- **ç»´æŠ¤æˆæœ¬**: æ¨¡å‹æ›´æ–°å’Œç›‘æ§
```

### ç”¨æˆ·ä½“éªŒè®¾è®¡
```markdown
# UXè®¾è®¡è¦ç‚¹
- **å“åº”æ—¶é—´**: å¤šæ¨¡æ€å¤„ç†é€Ÿåº¦
- **ç»“æœè´¨é‡**: è·¨æ¨¡æ€ç†è§£å‡†ç¡®æ€§
- **äº¤äº’æ–¹å¼**: è‡ªç„¶äº¤äº’è®¾è®¡
- **é”™è¯¯å¤„ç†**: ä¼˜é›…çš„é”™è¯¯å¤„ç†
```

### å•†ä¸šæ¨¡å¼
```markdown
# å•†ä¸šæ¨¡å¼é€‰æ‹©
- **APIæœåŠ¡**: æŒ‰è°ƒç”¨æ¬¡æ•°è®¡è´¹
- **ä¼ä¸šå®šåˆ¶**: è¡Œä¸šè§£å†³æ–¹æ¡ˆ
- **å¹³å°åŒ–**: å¤šæ¨¡æ€èƒ½åŠ›å¹³å°
- **å‚ç›´åº”ç”¨**: ç‰¹å®šè¡Œä¸šæ·±åº¦åº”ç”¨
```

## ğŸ”— ç›¸å…³æ¦‚å¿µ

- [[å›¾åƒTokenåŒ–]] - è§†è§‰æ•°æ®çš„TokenåŒ–æŠ€æœ¯
- [[å¤šæ¨¡æ€æ¨¡å‹å…¨æ™¯]] - å¤šæ¨¡æ€æ¨¡å‹çš„æ•´ä½“æ¶æ„
- [[å¤šæ¨¡æ€ç†è§£èƒ½åŠ›]] - å¤šæ¨¡æ€ç³»ç»Ÿçš„èƒ½åŠ›å»ºè®¾
- [[å¤šæ¨¡æ€å‘å±•å†ç¨‹]] - æŠ€æœ¯æ¼”è¿›è·¯çº¿

## ğŸ“ å®è·µå»ºè®®

### æŠ€æœ¯å®æ–½
```markdown
# å®æ–½å»ºè®®
1. **æ•°æ®è´¨é‡**: é«˜è´¨é‡å¤šæ¨¡æ€æ•°æ®
2. **é¢„è®­ç»ƒ**: åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹
3. **å¾®è°ƒ**: é’ˆå¯¹ç‰¹å®šåœºæ™¯å¾®è°ƒ
4. **è¯„ä¼°**: å»ºç«‹å®Œå–„è¯„ä¼°ä½“ç³»
```

### äº§å“å»ºè®®
```markdown
# äº§å“å»ºè®®
1. **åœºæ™¯éªŒè¯**: éªŒè¯æŠ€æœ¯ä»·å€¼
2. **ç”¨æˆ·ç ”ç©¶**: æ·±å…¥ç”¨æˆ·éœ€æ±‚
3. **MVPéªŒè¯**: æœ€å°å¯è¡Œäº§å“éªŒè¯
4. **è¿­ä»£ä¼˜åŒ–**: æŒç»­ä¼˜åŒ–ä½“éªŒ
```

---

*æ ‡ç­¾ï¼š#å¤šæ¨¡æ€ #èåˆæŠ€æœ¯ #æ·±åº¦å­¦ä¹  #AIäº§å“ç»ç†*
*ç›¸å…³é¡¹ç›®ï¼š[[AIäº§å“ç»ç†æŠ€æœ¯æ ˆé¡¹ç›®]]*
*å­¦ä¹ çŠ¶æ€ï¼š#æŠ€æœ¯åŸç† ğŸŸ¡ #åº”ç”¨å®è·µ ğŸ”´*