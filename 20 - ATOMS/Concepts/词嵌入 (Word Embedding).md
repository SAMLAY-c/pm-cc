# 概念: 词嵌入 (Word Embedding)

**标签**: #技术概念/NLP #核心技术
**来源**: [[MOC - LLM如何理解文字]]

---

> [!abstract] 核心概念
> 词嵌入（Word Embedding）是一种将离散的、无数学关系的Token ID，转换为连续的、蕴含丰富语义信息的高维向量（Vector）的技术。这是让模型从"识别文字"到"理解语义"的飞跃。

### 核心价值
- **赋予数字意义**: 向量在多维"意义空间"中的位置，代表了Token的语义。
- **捕捉词汇关系**: 
    - 意思相近的词，其向量在空间中的距离也相近（如"国王"与"女王"）。
    - 能够学习到抽象的类比关系（如 `向量("国王") - 向量("男人") + 向量("女人") ≈ 向量("女王")`）。

### 主要词嵌入技术
- **Word2Vec**: Google开发的基于上下文预测的词嵌入
- **GloVe**: 基于全局词频统计的词嵌入
- **FastText**: 处理子词信息，能更好地处理未登录词
- **Contextual Embeddings**: 如BERT，根据上下文动态生成词向量

### 向量维度选择
- **小型应用**: 100-300维
- **中型应用**: 300-768维
- **大型模型**: 768-4096维

### 质询与思辨
> [!question] 我的质询
> - 词嵌入向量的"维度"是多少，这个数字是如何决定的？维度越高，能表示的语义信息越丰富，但计算成本也越高。这在产品设计中是一个需要权衡的trade-off。
> - 预训练的词嵌入（如Word2Vec, GloVe）可能带有训练数据中的社会偏见（如性别、种族歧视）。作为产品经理，如何意识到并缓解这种"算法偏见"带来的负面影响？

### 产品设计启示
> [!idea] 产品经理思考
> - **性能vs精度**: 高维度带来更好效果但更高计算成本
> - **偏见检测**: 需要建立词嵌入偏见的检测机制
> - **领域适配**: 通用词嵌入vs领域特定词嵌入的选择
> - **更新策略**: 词嵌入如何随语言演化而更新

### 实际应用场景
- **推荐系统**: 基于语义相似度的内容推荐
- **搜索优化**: 语义搜索而非关键词匹配
- **内容分类**: 基于语义内容的自动分类
- **对话系统**: 理解用户意图和上下文

### 技术挑战
- **多语言支持**: 不同语言的语义空间对齐
- **新词处理**: 如何为新生词汇生成合理的向量
- **上下文理解**: 静态词嵌入vs动态上下文嵌入
- **计算效率**: 大规模向量运算的优化

### 行动项
- [ ] 研究不同词嵌入技术在教育领域的应用效果
- [ ] 设计词嵌入偏见的检测和缓解方案
- [ ] 评估词嵌入维度对产品性能的影响

**学习状态**: #质询中  
**创建日期**: <% tp.date.now("YYYY-MM-DD") %>