# 概念: 分词 (Tokenization)

**标签**: #技术概念/NLP #数据预处理
**来源**: [[MOC - LLM如何理解文字]]

---

> [!abstract] 核心概念
> 分词（Tokenization）是将连续的文本字符串，拆解成模型能够理解的有意义的最小单元（Token）的过程。这是自然语言处理的第一步，也是至关重要的一步。

### 不同语言的分词策略
- **英文**: 相对简单，通常可以按空格和标点符号进行拆分。
- **中文**: 更复杂，因为词与词之间没有天然的分隔符，需要依赖复杂的算法（如BPE, WordPiece）来识别词汇边界。

### 常见分词算法
- **BPE (Byte Pair Encoding)**: 字节对编码，通过合并频繁出现的字符对来构建词汇表
- **WordPiece**: Google开发的类似BPE的算法，但基于概率而非频率
- **SentencePiece**: 无需预处理，直接处理原始文本的分词算法

### 分词的挑战
- **新词处理**: 如何处理训练时未见过的词汇
- **歧义消解**: 同一个字符序列可能有多种分词方式
- **跨语言一致性**: 不同语言的分词标准和粒度差异

### 质询与思辨 (Interrogation & Reflection)
> [!question] 我的质询
> - **分词的粒度如何影响模型性能？** 分得太细（如按单个字符）和分得太粗（如按长词）各有什么优劣？
> - 对于一个新出现的网络热词（如"泰酷辣"），分词器如果没见过，会如何处理？这是否是导致模型有时无法理解"梗"的原因之一？
> - 作为产品经理，在设计一个垂直领域的AI应用时，我们是否需要为这个领域定制一个专属的分词器，来更好地处理行业术语？

### 实际应用思考
> [!idea] 产品设计启示
> - **教育类AI应用**: 需要特别注意专业术语的分词准确性
> - **社交媒体分析**: 需要考虑网络用语和表情符号的处理
> - **多语言产品**: 需要针对不同语言优化分词策略

### 相关技术概念
- [[词汇表映射 (Vocabulary Mapping)]]: 分词后的下一步
- [[词嵌入 (Word Embedding)]]: Token的语义表示
- [[模型处理 (Model Processing)]]: 最终的理解过程

### 行动项
- [ ] 调研主流LLM的分词器实现方式
- [ ] 分析B站内容中的特殊词汇对分词的挑战
- [ ] 思考如何优化K12教育领域的分词策略

**学习状态**: #质询中  
**创建日期**: <% tp.date.now("YYYY-MM-DD") %>